{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6142bc6d-8831-4c70-b701-a99805103cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article blackassign0001 saved to extracted_articles\\blackassign0001.txt\n",
      "Article blackassign0002 saved to extracted_articles\\blackassign0002.txt\n",
      "Article blackassign0003 saved to extracted_articles\\blackassign0003.txt\n",
      "Article blackassign0004 saved to extracted_articles\\blackassign0004.txt\n",
      "Article blackassign0005 saved to extracted_articles\\blackassign0005.txt\n",
      "Article blackassign0006 saved to extracted_articles\\blackassign0006.txt\n",
      "Article blackassign0007 saved to extracted_articles\\blackassign0007.txt\n",
      "Article blackassign0008 saved to extracted_articles\\blackassign0008.txt\n",
      "Article blackassign0009 saved to extracted_articles\\blackassign0009.txt\n",
      "Article blackassign0010 saved to extracted_articles\\blackassign0010.txt\n",
      "Article blackassign0011 saved to extracted_articles\\blackassign0011.txt\n",
      "Article blackassign0012 saved to extracted_articles\\blackassign0012.txt\n",
      "Article blackassign0013 saved to extracted_articles\\blackassign0013.txt\n",
      "Article blackassign0014 saved to extracted_articles\\blackassign0014.txt\n",
      "Article blackassign0015 saved to extracted_articles\\blackassign0015.txt\n",
      "Article blackassign0016 saved to extracted_articles\\blackassign0016.txt\n",
      "Article blackassign0017 saved to extracted_articles\\blackassign0017.txt\n",
      "Article blackassign0018 saved to extracted_articles\\blackassign0018.txt\n",
      "Article blackassign0019 saved to extracted_articles\\blackassign0019.txt\n",
      "Article blackassign0020 saved to extracted_articles\\blackassign0020.txt\n",
      "Article blackassign0021 saved to extracted_articles\\blackassign0021.txt\n",
      "Article blackassign0022 saved to extracted_articles\\blackassign0022.txt\n",
      "Article blackassign0023 saved to extracted_articles\\blackassign0023.txt\n",
      "Article blackassign0024 saved to extracted_articles\\blackassign0024.txt\n",
      "Article blackassign0025 saved to extracted_articles\\blackassign0025.txt\n",
      "Article blackassign0026 saved to extracted_articles\\blackassign0026.txt\n",
      "Article blackassign0027 saved to extracted_articles\\blackassign0027.txt\n",
      "Article blackassign0028 saved to extracted_articles\\blackassign0028.txt\n",
      "Article blackassign0029 saved to extracted_articles\\blackassign0029.txt\n",
      "Article blackassign0030 saved to extracted_articles\\blackassign0030.txt\n",
      "Article blackassign0031 saved to extracted_articles\\blackassign0031.txt\n",
      "Article blackassign0032 saved to extracted_articles\\blackassign0032.txt\n",
      "Article blackassign0033 saved to extracted_articles\\blackassign0033.txt\n",
      "Article blackassign0034 saved to extracted_articles\\blackassign0034.txt\n",
      "Article blackassign0035 saved to extracted_articles\\blackassign0035.txt\n",
      "Failed to retrieve content from https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/. Status code: 404\n",
      "Article blackassign0037 saved to extracted_articles\\blackassign0037.txt\n",
      "Article blackassign0038 saved to extracted_articles\\blackassign0038.txt\n",
      "Article blackassign0039 saved to extracted_articles\\blackassign0039.txt\n",
      "Article blackassign0040 saved to extracted_articles\\blackassign0040.txt\n",
      "Article blackassign0041 saved to extracted_articles\\blackassign0041.txt\n",
      "Article blackassign0042 saved to extracted_articles\\blackassign0042.txt\n",
      "Article blackassign0043 saved to extracted_articles\\blackassign0043.txt\n",
      "Article blackassign0044 saved to extracted_articles\\blackassign0044.txt\n",
      "Article blackassign0045 saved to extracted_articles\\blackassign0045.txt\n",
      "Article blackassign0046 saved to extracted_articles\\blackassign0046.txt\n",
      "Article blackassign0047 saved to extracted_articles\\blackassign0047.txt\n",
      "Article blackassign0048 saved to extracted_articles\\blackassign0048.txt\n",
      "Failed to retrieve content from https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/. Status code: 404\n",
      "Article blackassign0050 saved to extracted_articles\\blackassign0050.txt\n",
      "Article blackassign0051 saved to extracted_articles\\blackassign0051.txt\n",
      "Article blackassign0052 saved to extracted_articles\\blackassign0052.txt\n",
      "Article blackassign0053 saved to extracted_articles\\blackassign0053.txt\n",
      "Article blackassign0054 saved to extracted_articles\\blackassign0054.txt\n",
      "Article blackassign0055 saved to extracted_articles\\blackassign0055.txt\n",
      "Article blackassign0056 saved to extracted_articles\\blackassign0056.txt\n",
      "Article blackassign0057 saved to extracted_articles\\blackassign0057.txt\n",
      "Article blackassign0058 saved to extracted_articles\\blackassign0058.txt\n",
      "Article blackassign0059 saved to extracted_articles\\blackassign0059.txt\n",
      "Article blackassign0060 saved to extracted_articles\\blackassign0060.txt\n",
      "Article blackassign0061 saved to extracted_articles\\blackassign0061.txt\n",
      "Article blackassign0062 saved to extracted_articles\\blackassign0062.txt\n",
      "Article blackassign0063 saved to extracted_articles\\blackassign0063.txt\n",
      "Article blackassign0064 saved to extracted_articles\\blackassign0064.txt\n",
      "Article blackassign0065 saved to extracted_articles\\blackassign0065.txt\n",
      "Article blackassign0066 saved to extracted_articles\\blackassign0066.txt\n",
      "Article blackassign0067 saved to extracted_articles\\blackassign0067.txt\n",
      "Article blackassign0068 saved to extracted_articles\\blackassign0068.txt\n",
      "Article blackassign0069 saved to extracted_articles\\blackassign0069.txt\n",
      "Article blackassign0070 saved to extracted_articles\\blackassign0070.txt\n",
      "Article blackassign0071 saved to extracted_articles\\blackassign0071.txt\n",
      "Article blackassign0072 saved to extracted_articles\\blackassign0072.txt\n",
      "Article blackassign0073 saved to extracted_articles\\blackassign0073.txt\n",
      "Article blackassign0074 saved to extracted_articles\\blackassign0074.txt\n",
      "Article blackassign0075 saved to extracted_articles\\blackassign0075.txt\n",
      "Article blackassign0076 saved to extracted_articles\\blackassign0076.txt\n",
      "Article blackassign0077 saved to extracted_articles\\blackassign0077.txt\n",
      "Article blackassign0078 saved to extracted_articles\\blackassign0078.txt\n",
      "Article blackassign0079 saved to extracted_articles\\blackassign0079.txt\n",
      "Article blackassign0080 saved to extracted_articles\\blackassign0080.txt\n",
      "Article blackassign0081 saved to extracted_articles\\blackassign0081.txt\n",
      "Article blackassign0082 saved to extracted_articles\\blackassign0082.txt\n",
      "Article blackassign0083 saved to extracted_articles\\blackassign0083.txt\n",
      "Article blackassign0084 saved to extracted_articles\\blackassign0084.txt\n",
      "Article blackassign0085 saved to extracted_articles\\blackassign0085.txt\n",
      "Article blackassign0086 saved to extracted_articles\\blackassign0086.txt\n",
      "Article blackassign0087 saved to extracted_articles\\blackassign0087.txt\n",
      "Article blackassign0088 saved to extracted_articles\\blackassign0088.txt\n",
      "Article blackassign0089 saved to extracted_articles\\blackassign0089.txt\n",
      "Article blackassign0090 saved to extracted_articles\\blackassign0090.txt\n",
      "Article blackassign0091 saved to extracted_articles\\blackassign0091.txt\n",
      "Article blackassign0092 saved to extracted_articles\\blackassign0092.txt\n",
      "Article blackassign0093 saved to extracted_articles\\blackassign0093.txt\n",
      "Article blackassign0094 saved to extracted_articles\\blackassign0094.txt\n",
      "Article blackassign0095 saved to extracted_articles\\blackassign0095.txt\n",
      "Article blackassign0096 saved to extracted_articles\\blackassign0096.txt\n",
      "Article blackassign0097 saved to extracted_articles\\blackassign0097.txt\n",
      "Article blackassign0098 saved to extracted_articles\\blackassign0098.txt\n",
      "Article blackassign0099 saved to extracted_articles\\blackassign0099.txt\n",
      "Article blackassign0100 saved to extracted_articles\\blackassign0100.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def extract_article(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract the article text\n",
    "        article_text = ''\n",
    "        for paragraph in soup.find_all('p'):\n",
    "            article_text += paragraph.text.strip() + ' '\n",
    "\n",
    "        return article_text\n",
    "    else:\n",
    "        print(f\"Failed to retrieve content from {url}. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def process_articles(input_excel):\n",
    "    # Read the input Excel file\n",
    "    df = pd.read_excel(input_excel)\n",
    "\n",
    "    # Create a directory to store the text files\n",
    "    output_directory = 'extracted_articles'\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # Process each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        url_id = row['URL_ID']\n",
    "        url = row['URL']\n",
    "\n",
    "        # Extract the article text\n",
    "        article_text = extract_article(url)\n",
    "\n",
    "        if article_text:\n",
    "            # Save the article text to a text file\n",
    "            output_file_path = os.path.join(output_directory, f'{url_id}.txt')\n",
    "            with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(article_text)\n",
    "            print(f\"Article {url_id} saved to {output_file_path}\")\n",
    "\n",
    "# Example usage\n",
    "input_excel = 'input.xlsx'\n",
    "process_articles(input_excel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb54b842-a13b-4dfb-ab83-4484faf7239d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Abhinandan\n",
      "[nltk_data]     Patra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\Abhinandan Patra\\AppData\\Local\\Temp\\ipykernel_13892\\1223863646.py:164: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  result_df = pd.concat([result_df, pd.DataFrame([analysis_result], columns=result_columns)], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found for URL_ID blackassign0036. Skipping.\n",
      "File not found for URL_ID blackassign0049. Skipping.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import chardet\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def read_word_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        words = file.read().splitlines()\n",
    "    return set(words)\n",
    "\n",
    "\n",
    "def count_syllables(word):\n",
    "    # Define a regular expression to match vowels\n",
    "    vowels = re.compile(r'[aeiouy]')\n",
    "\n",
    "    # Handle exceptions for words ending with \"es\" or \"ed\"\n",
    "    if word.endswith(('es', 'ed')):\n",
    "        return 1\n",
    "\n",
    "    # Count the number of vowels in the word\n",
    "    syllables = len(vowels.findall(word.lower()))\n",
    "\n",
    "    return max(1, syllables)  # Ensure at least one syllable\n",
    "\n",
    "# Load Positive and Negative Dictionaries\n",
    "positive_words = read_word_file('positive-words.txt')\n",
    "negative_words = read_word_file('utf-8-negative-words.txt')\n",
    "\n",
    "# Load Stopwords\n",
    "stopwords_auditor = read_word_file('StopWords_Auditor.txt')\n",
    "stopwords_currencies = read_word_file('utf-8-StopWords_Currencies.txt')\n",
    "stopwords_date_and_numbers = read_word_file('StopWords_DatesandNumbers.txt')\n",
    "stopwords_generic = read_word_file('StopWords_Generic.txt')\n",
    "stopwords_generic_long = read_word_file('StopWords_GenericLong.txt')\n",
    "stopwords_geographic = read_word_file('StopWords_Geographic.txt')\n",
    "stopwords_names = read_word_file('StopWords_Names.txt')\n",
    "\n",
    "\n",
    "\n",
    "def combine_stopwords(*stopword_lists):\n",
    "    return set(word for stopword_list in stopword_lists for word in stopword_list)\n",
    "\n",
    "all_stopwords = combine_stopwords(\n",
    "    stopwords_auditor,\n",
    "    stopwords_currencies,\n",
    "    stopwords_date_and_numbers,\n",
    "    stopwords_generic,\n",
    "    stopwords_generic_long,\n",
    "    stopwords_geographic,\n",
    "    stopwords_names\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def perform_textual_analysis(text):\n",
    "\n",
    "    # Tokenize the text into words\n",
    "     words = word_tokenize(text)\n",
    "\n",
    "    # Remove custom stopwords\n",
    "     filtered_words = [word.lower() for word in words if word.isalnum() and word.lower() not in all_stopwords]\n",
    "\n",
    "    # Calculate word frequency\n",
    "     word_freq = FreqDist(filtered_words)\n",
    "    # Tokenize the text into words\n",
    "     words = word_tokenize(text)\n",
    "\n",
    "    # Remove custom stopwords\n",
    "     filtered_words = [word.lower() for word in words if word.isalnum() and word.lower() not in all_stopwords]\n",
    "\n",
    "    # Calculate word frequency\n",
    "     word_freq = FreqDist(filtered_words)\n",
    "\n",
    "    # Calculate the number of sentences\n",
    "     sentences = sent_tokenize(text)\n",
    "     num_sentences = len(sentences)\n",
    "\n",
    "    # Additional variables for textual analysis\n",
    "     avg_sentence_length = sum(len(sent.split()) for sent in sentences) / num_sentences\n",
    "     percentage_of_complex_words = len([word for word in filtered_words if len(word) > 2]) / len(filtered_words) * 100\n",
    "     fog_index = 0.4 * (avg_sentence_length + percentage_of_complex_words)\n",
    "\n",
    "     complex_word_count = len([word for word in filtered_words if len(word) > 2])\n",
    "     word_count = len(words)\n",
    "\n",
    "    # Syllables per word and avg word length are not computed in this example\n",
    "     syllables_per_word = sum(count_syllables(word) for word in filtered_words) / len(filtered_words) if filtered_words else 0\n",
    "     avg_word_length = sum(len(word) for word in filtered_words) / len(filtered_words) if filtered_words else 0\n",
    "\n",
    "     avg_words_per_sentence = word_count / num_sentences if num_sentences != 0 else 0\n",
    "    # Personal pronouns count\n",
    "     personal_pronouns = sum(word.lower() in ['i', 'me', 'my', 'mine', 'myself', 'you', 'your', 'yours', 'yourself',\n",
    "                                            'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself',\n",
    "                                            'it', 'its', 'itself', 'we', 'us', 'our', 'ours', 'ourselves',\n",
    "                                            'they', 'them', 'their', 'theirs', 'themselves'] for word in words)\n",
    "\n",
    "    # Positive Score, Negative Score, Polarity Score, Subjectivity Score calculation\n",
    "     positive_score = sum(1 for word in filtered_words if word in positive_words)\n",
    "     negative_score = sum(1 for word in filtered_words if word in negative_words)\n",
    "\n",
    "    # Avoid division by zero\n",
    "     total_words_after_cleaning = len(filtered_words) or 1\n",
    "\n",
    "     polarity_score = (positive_score - negative_score) / (positive_score + negative_score + 1e-6)\n",
    "     subjectivity_score = (positive_score + negative_score) / total_words_after_cleaning\n",
    "\n",
    "     return {\n",
    "        'Positive Score': positive_score,\n",
    "        'Negative Score': negative_score,\n",
    "        'Polarity Score': polarity_score,\n",
    "        'Subjectivity Score': subjectivity_score,\n",
    "        'Avg Sentence Length': avg_sentence_length,\n",
    "        'Percentage of Complex Words': percentage_of_complex_words,\n",
    "        'Fog Index': fog_index,\n",
    "        'Avg Words per Sentence': avg_words_per_sentence,\n",
    "        'Complex Word Count': complex_word_count,\n",
    "        'Word Count': word_count,\n",
    "        'Syllables per Word': syllables_per_word,\n",
    "        'Personal Pronouns': personal_pronouns,\n",
    "        'Avg Word Length': avg_word_length,\n",
    "     }\n",
    "\n",
    "\n",
    "\n",
    "def process_articles_and_save_results(input_excel, output_excel):\n",
    "    # Read the input Excel file\n",
    "    df = pd.read_excel(input_excel)\n",
    "\n",
    "    # Create an empty DataFrame to store results\n",
    "    result_columns = [\n",
    "        'URL_ID',\n",
    "        'URL',\n",
    "        'Positive Score',\n",
    "        'Negative Score',\n",
    "        'Polarity Score',\n",
    "        'Subjectivity Score',\n",
    "        'Avg Sentence Length',\n",
    "        'Percentage of Complex Words',\n",
    "        'Fog Index',\n",
    "        'Avg Words per Sentence',\n",
    "        'Complex Word Count',\n",
    "        'Word Count',\n",
    "        'Syllables per Word',\n",
    "        'Personal Pronouns',\n",
    "        'Avg Word Length',\n",
    "    ]\n",
    "    result_df = pd.DataFrame(columns=result_columns)\n",
    "\n",
    "    # Process each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        url_id = row['URL_ID']\n",
    "        file_path = f'extracted_articles/{url_id}.txt'  # Update the path accordingly\n",
    "\n",
    "        try:\n",
    "            # Read the article text from the file\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                article_text = file.read()\n",
    "\n",
    "            # Perform textual analysis\n",
    "            analysis_result = perform_textual_analysis(article_text)\n",
    "\n",
    "            analysis_result['URL_ID'] = url_id\n",
    "            analysis_result['URL'] = row['URL']  # Assuming 'URL' is the column name in your input DataFrame\n",
    "\n",
    "            # Append the results to the DataFrame\n",
    "            result_df = pd.concat([result_df, pd.DataFrame([analysis_result], columns=result_columns)], ignore_index=True)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found for URL_ID {url_id}. Skipping.\")\n",
    "\n",
    "    # Save the DataFrame to an Excel file\n",
    "    result_df.to_excel(output_excel, index=False)\n",
    "\n",
    "input_excel = 'input.xlsx'\n",
    "output_excel = 'Output Data Structure.xlsx'\n",
    "process_articles_and_save_results(input_excel, output_excel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0205b4-e2e1-4931-8057-c1dd21b2e360",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
